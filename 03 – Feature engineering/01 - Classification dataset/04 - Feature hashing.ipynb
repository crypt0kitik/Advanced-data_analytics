{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - High cardinality management: Feature hashing \n",
    "\n",
    "Feature Hashing, also known as the \"hashing trick,\" is an efficient way to handle high-cardinality categorical features. It maps categories to a fixed number of bins using a hash function, reducing dimensionality without explicitly creating columns for each unique category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"bank_numeric.csv\")\n",
    "\n",
    "# Define features and target\n",
    "target_column = \"deposit\"\n",
    "X = df.drop(columns=[target_column])\n",
    "y = df[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Categorical Columns for Feature Hashing: []\n"
     ]
    }
   ],
   "source": [
    "# Identify high-cardinality categorical columns\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns\n",
    "print(\"\\nCategorical Columns for Feature Hashing:\", categorical_columns.tolist())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# the code is attempting to identify categorical columns\n",
    "# but there are no columns in the dataset \n",
    "# have the data type object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential Categorical Columns: ['marital', 'education', 'default', 'housing', 'loan', 'contact', 'previous', 'poutcome']\n"
     ]
    }
   ],
   "source": [
    "# Identify columns with a small number of unique values\n",
    "# threshold is 10\n",
    "potential_categorical_columns = [col for col in X.columns if X[col].nunique() <= 10]\n",
    "print(\"Potential Categorical Columns:\", potential_categorical_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marital      object\n",
      "education    object\n",
      "default      object\n",
      "housing      object\n",
      "loan         object\n",
      "contact      object\n",
      "previous     object\n",
      "poutcome     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert identified columns to object type\n",
    "categorical_columns = ['marital', 'education', 'default', 'housing', 'loan', 'contact', 'previous', 'poutcome']\n",
    "X[categorical_columns] = X[categorical_columns].astype('object')\n",
    "\n",
    "# Confirm the changes\n",
    "print(X.dtypes[categorical_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics Before Feature Hashing:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.90      0.84       915\n",
      "           1       0.83      0.65      0.73       663\n",
      "\n",
      "    accuracy                           0.80      1578\n",
      "   macro avg       0.81      0.78      0.78      1578\n",
      "weighted avg       0.80      0.80      0.79      1578\n",
      "\n",
      "\n",
      "Model overall accuracy (Before Feature Hashing): 79.78%\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Logistic Regression BEFORE Feature Hashing\n",
    "# For simplicity, drop the high-cardinality categorical columns\n",
    "X_train_no_hash = X_train.drop(columns=categorical_columns)\n",
    "X_test_no_hash = X_test.drop(columns=categorical_columns)\n",
    "\n",
    "# Train logistic regression on the dataset without hashing\n",
    "log_reg_no_hash = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg_no_hash.fit(X_train_no_hash, y_train)\n",
    "\n",
    "# Predict and evaluate metrics\n",
    "y_pred_no_hash = log_reg_no_hash.predict(X_test_no_hash)\n",
    "print(\"\\nMetrics Before Feature Hashing:\")\n",
    "print(classification_report(y_test, y_pred_no_hash))\n",
    "acc_no_hash = accuracy_score(y_test, y_pred_no_hash)\n",
    "print(\"\\nModel overall accuracy (Before Feature Hashing): {:.2f}%\".format(acc_no_hash * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Shape After Feature Hashing: (4781, 18)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Apply Feature Hashing\n",
    "def apply_feature_hashing(df, columns, n_features):\n",
    "    hasher = FeatureHasher(n_features=n_features, input_type='string')\n",
    "    \n",
    "    # Combine categorical columns row-wise (iterable of iterables of strings)\n",
    "    combined_categorical = df[columns].astype(str).values.tolist()\n",
    "    \n",
    "    # Apply FeatureHasher to the combined categorical columns\n",
    "    hashed_features = hasher.transform(combined_categorical).toarray()\n",
    "    \n",
    "    # Return hashed features as a DataFrame\n",
    "    hashed_df = pd.DataFrame(hashed_features, columns=[f\"hash_{i}\" for i in range(n_features)])\n",
    "    \n",
    "    # Concatenate hashed features with remaining numeric columns\n",
    "    return pd.concat([df.drop(columns=columns), hashed_df], axis=1)\n",
    "\n",
    "# Number of hashed features\n",
    "n_hash_features = 10\n",
    "\n",
    "# Apply hashing to training and test sets\n",
    "X_train_hashed = apply_feature_hashing(X_train, categorical_columns, n_hash_features)\n",
    "X_test_hashed = apply_feature_hashing(X_test, categorical_columns, n_hash_features)\n",
    "\n",
    "print(\"\\nDataset Shape After Feature Hashing:\", X_train_hashed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values\n",
    "X_train_hashed = X_train_hashed.dropna()\n",
    "y_train = y_train[X_train_hashed.index]  # Ensure target matches after dropping rows\n",
    "\n",
    "X_test_hashed = X_test_hashed.dropna()\n",
    "y_test = y_test[X_test_hashed.index]  # Ensure target matches after dropping rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics After Feature Hashing:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.78      0.88       504\n",
      "\n",
      "    accuracy                           0.78       504\n",
      "   macro avg       0.50      0.39      0.44       504\n",
      "weighted avg       1.00      0.78      0.88       504\n",
      "\n",
      "\n",
      "Model overall accuracy (After Feature Hashing): 78.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\e1003118\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\e1003118\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\e1003118\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\e1003118\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Logistic Regression AFTER Feature Hashing\n",
    "log_reg_hashed = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg_hashed.fit(X_train_hashed, y_train)\n",
    "\n",
    "# Predict and evaluate metrics\n",
    "y_pred_hashed = log_reg_hashed.predict(X_test_hashed)\n",
    "print(\"\\nMetrics After Feature Hashing:\")\n",
    "print(classification_report(y_test, y_pred_hashed))\n",
    "acc_hashed = accuracy_score(y_test, y_pred_hashed)\n",
    "print(\"\\nModel overall accuracy (After Feature Hashing): {:.2f}%\".format(acc_hashed * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insights:\n",
    "\n",
    "# first of all,\n",
    "# I had to change the types of columns\n",
    "# because I did not have objects for hashing\n",
    "\n",
    "# secondly,\n",
    "# after hashing I had some Nan values\n",
    "# which I had to delete \n",
    "\n",
    "# thirdly,\n",
    "# this technique did not improve metrics\n",
    "# even made it worse in my case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
